{
  "metadata": {
    "name": "Spark_Classification_Election",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# US Election Prediction - Random Forest\n\n-- County facts dictionary is used for analysis by you (county_facts columns are codes)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\r\nrm -r /var/tmp/*.csv\r\nwget -q https://raw.githubusercontent.com/tofighi/dataset/main/big-data/election2016/county_facts_dictionary.csv -O /var/tmp/county_facts.csv\r\nwget -q https://raw.githubusercontent.com/tofighi/dataset/main/big-data/election2016/primary_results.csv -O /var/tmp/primary_results.csv\r\nwget -q https://raw.githubusercontent.com/tofighi/dataset/main/big-data/election2016/county_facts_dictionary.csv -O /var/tmp/county_facts_dictionary.csv\r\nls -lah /var/tmp"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\nimport org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.sql.types.{IntegerType, DoubleType}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val results \u003d spark.read\r\n .format(\"csv\")\r\n .option(\"header\", \"true\")\r\n .load(\"file:///var/tmp/primary_results.*\")\r\n\r\nval county \u003d spark.read\r\n .format(\"csv\")\r\n .option(\"header\", \"true\")\r\n  .load(\"file:///var/tmp/county_facts.*\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val results_rep \u003d results.filter(col(\"party\") \u003d\u003d\u003d \"Republican\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Join results with county\r\nval join_expr \u003d results_rep.col(\"fips\") \u003d\u003d\u003d county.col(\"fips\").cast(IntegerType)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "-- To load the information, we need to make sense of the file, we use the description in county_facts_dictionry.csv\n-- White, hispanic, college are all represented by percentages\n-- Income represented the medium household income of the county\n-- pop_density \u003d population per square mile\n-- Here we are joining the results files with the county file based on the county id\n-- we\u0027ll cast to the respective types, all numberical columns will be doubles"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val raw_dataset \u003d results_rep.join(county, join_expr).drop(county.col(\"fips\")).select(\r\n col(\"fips\").alias(\"county_id\"), \r\n col(\"area_name\").alias(\"county\"), \r\n col(\"RHI125214\").cast(DoubleType).alias(\"white\"), \r\n col(\"RHI725214\").cast(DoubleType).alias(\"hispanic\"), \r\n col(\"INC110213\").cast(DoubleType).alias(\"income\"), \r\n col(\"EDU685213\").cast(DoubleType).alias(\"college\"),\r\n col(\"POP060210\").cast(DoubleType).alias(\"pop_density\"),\r\n col(\"candidate\"), \r\n col(\"fraction_votes\").cast(DoubleType))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "raw_dataset.show(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Group by county \u0026 Rank by votes  \r\n-- We don\u0027t have a winner column but we can derive it (recall feature engineering)\r\n-- We can group the records by county, and within each county we rank the records by fraction_votes\r\n-- Next we filter the records with rank 1\r\n-- We are using a window function to group the records by county"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval windowSpec \u003d Window.partitionBy(col(\"county_id\")).orderBy(col(\"fraction_votes\").desc)\r\nval ranking \u003d rank().over(windowSpec)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "-- Next we select the columns that we need\n-- And we fetch just the records with rank 1\n-- We are also filtering out a candidate called Ben Carson\n-- Ben Carson dropped out and it causes issues here\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\nval dataset \u003d raw_dataset.select(col(\"county_id\"), col(\"county\"), col(\"white\"), col(\"hispanic\"), \r\n col(\"income\"), col(\"college\"), col(\"pop_density\"), col(\"candidate\"), col(\"fraction_votes\"), ranking.alias(\"rank\"))\r\n .filter(col(\"rank\") \u003d\u003d\u003d lit(1) and !(col(\"candidate\") \u003d\u003d\u003d lit(\"Ben Carson\")))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "-- Split our dataset into training and test data typical 80 20\n-- we give a seed so we have the same random data in each set"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval Array(trainingData, testData) \u003d dataset.randomSplit(Array(0.8, 0.2), 754) "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Index the candidate (String) column\n-- Our candidate column is of type string --\u003e not very good for ML algorithms\n-- We use the StringIndexer to index the column values to number \n-- Ex Donald Trump \u003d 1, Ted Cruz \u003d 2, etc...\n-- We call this new column \"candidate_index\"\n-- note this is the reason we get rid of Ben Carson --\u003e he only has two values, can end up in test set \n-- model will train it based on the training set and will give error (what is ben carson)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval indexer \u003d new StringIndexer()\r\n  .setInputCol(\"candidate\")\r\n  .setOutputCol(\"candidate_indexed\")    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Assemble all features\r\n-- Next we assemble our features using vector assembler, this time we have more than one feature"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval assembler \u003d new VectorAssembler()\n .setInputCols(Array(\"white\", \"hispanic\", \"income\", \"college\", \"pop_density\"))\n .setOutputCol(\"assembled-features\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Random Forest  \n-- We now create a new random forest object \n-- give features (as a vector)\n-- give the label as candiate_indexed (can\u0027t use the string column)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval rf \u003d new RandomForestClassifier()\r\n .setFeaturesCol(\"assembled-features\")\r\n .setLabelCol(\"candidate_indexed\")\r\n .setSeed(1234)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n### Set up pipeline\r\n-- Use pipepline to set our stages\r\n-- So our stages are the string indexer, the vector assembler and the random forest classifier object\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval pipeline \u003d new Pipeline()\n  .setStages(Array(indexer, assembler, rf))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Evaluate the model\n-- Next we provide the evaluator\n-- For regressoin we used RegressionEvaluator\n-- the metric accuracy was simply a percentage\n-- Here we are using MulticlassClassificationEvaluator\n-- we compared candidate_indexed ot the prediction column\n-- IF the match, prediction is good else it is unsuccession\n-- metric \"accuracy\" is basically percentage of accurate results\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval evaluator \u003d new MulticlassClassificationEvaluator()\r\n  .setLabelCol(\"candidate_indexed\")\r\n  .setPredictionCol(\"prediction\")\r\n  .setMetricName(\"accuracy\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Hyper parameters\n-- First is maxDepth, it\u0027s an array with two values, 5, 10 \n-- maxDepth puts a limit on how deep we can construct the tree\n-- Remember, we said that with decision trees main problem was overfitting the data\n-- random forest does a better job but still, there is an issue if we allow the tree to be too deep\n-- so good idea to set the max depth\n-- Second is impurity which we give as entropy (there are other statistical methods to select features)."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval paramGrid \u003d new ParamGridBuilder()  \r\n  .addGrid(rf.maxDepth, Array(3, 5))\r\n  .addGrid(rf.impurity, Array(\"entropy\",\"gini\")).build()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Cross validate model\n-- Next we tie everything together with cross-validator\n-- We set the pipeline for estimator\n-- Multiclassevaluator for evaluator\n-- Set the hyper parameters and the number of folds\n-- Cross validator will divide the training data set into 3\n-- Next each fold is coupled with the paramters for each type\n-- Fold 1 is tried with max depth 3 and entropy and then fold 1 is again tried but this time with max depth 5 and entrop\n-- Next fold 2 is tried with max depth 3 and entropy, and again with max depth 5 and entropy\n-- And so on ...\n-- In total, we will have 3 (folds) x 2 (depth) x 2 (algorithms) \u003d 12 models\n-- the best model is picked"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval cross_validator \u003d new CrossValidator()\r\n  .setEstimator(pipeline)\r\n  .setEvaluator(evaluator)\r\n  .setEstimatorParamMaps(paramGrid)\r\n  .setNumFolds(3)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Training\n-- Train the model on training data\n-- Gives us the best model from the 6 variations\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval cvModel \u003d cross_validator.fit(trainingData)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Predicting with test data\n-- Transform testData with predictions"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval predictions \u003d cvModel.transform(testData)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\npredictions.select(\"candidate\",\"candidate_indexed\",\"rawPrediction\").show(20)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Evaluating the model\n-- check with actual values and print accuracy\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval accuracy \u003d evaluator.evaluate(predictions)\r\nprintln(\"accuracy on test data \u003d \" + accuracy)"
    }
  ]
}