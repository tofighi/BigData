{"paragraphs":[{"text":"%md\n# Happiness Dataset\nThis is a Regression example for predicting a numerical value. We first need to download Happiness dataset and we are going to predict Happiness rank from Happiness score of a country.","user":"anonymous","dateUpdated":"2022-05-24T20:45:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Happiness Dataset</h1>\n<p>This is a Regression example for predicting a numerical value. We first need to download Happiness dataset and we are going to predict Happiness rank from Happiness score of a country.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653421528243_551540175","id":"20220524-194528_1656670317","dateCreated":"2022-05-24T19:45:28+0000","dateStarted":"2022-05-24T20:43:57+0000","dateFinished":"2022-05-24T20:43:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:56515"},{"text":"%sh\ncurl https://raw.githubusercontent.com/tofighi/dataset/main/big-data/happiness/happiness_2020.csv --output happiness.csv --silent\nmv happiness.csv  /var/tmp/","user":"anonymous","dateUpdated":"2022-05-24T20:13:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1653420736765_-1151811671","id":"20220524-193216_2080317810","dateCreated":"2022-05-24T19:32:16+0000","dateStarted":"2022-05-24T20:13:06+0000","dateFinished":"2022-05-24T20:13:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56516"},{"text":"%md\nJust a bunch of import statements, you can run these in a paste command\n","user":"anonymous","dateUpdated":"2022-05-24T20:13:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Just a bunch of import statements, you can run these in a paste command</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653422386757_-1754421419","id":"20220524-195946_1460509698","dateCreated":"2022-05-24T19:59:46+0000","dateStarted":"2022-05-24T20:13:07+0000","dateFinished":"2022-05-24T20:13:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56517"},{"text":"import org.apache.spark.sql.functions._\r\nimport org.apache.spark.ml.feature.{VectorAssembler}\r\nimport org.apache.spark.ml.Pipeline\r\nimport org.apache.spark.ml.regression.{LinearRegression}\r\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\r\nimport org.apache.spark.ml.evaluation.{RegressionEvaluator}\r\nimport org.apache.spark.ml.param.ParamMap\r\nimport org.apache.spark.sql.types.{DoubleType}","user":"anonymous","dateUpdated":"2022-05-24T20:13:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.sql.types.DoubleType\n"}]},"apps":[],"jobName":"paragraph_1653418825829_1260088154","id":"20220524-190025_1527682737","dateCreated":"2022-05-24T19:00:25+0000","dateStarted":"2022-05-24T20:13:07+0000","dateFinished":"2022-05-24T20:13:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56518"},{"text":"%md\n-- Loading our CSV file, note the inferSchema option being set to true","user":"anonymous","dateUpdated":"2022-05-24T20:13:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Loading our CSV file, note the inferSchema option being set to true</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653422401782_-152560287","id":"20220524-200001_1614000196","dateCreated":"2022-05-24T20:00:01+0000","dateStarted":"2022-05-24T20:13:07+0000","dateFinished":"2022-05-24T20:13:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56519"},{"text":"val data = spark.read\r\n .format(\"csv\")\r\n .option(\"sep\", \",\")\r\n .option(\"inferSchema\", \"true\")\r\n .option(\"header\", \"true\")\r\n .load(\"file:///var/tmp/happiness.csv\")","user":"anonymous","dateUpdated":"2022-05-24T20:13:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"data: org.apache.spark.sql.DataFrame = [Country name: string, Regional indicator: string ... 19 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=185","http://728940ef7f86:4040/jobs/job?id=186"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653419170136_-468493001","id":"20220524-190610_719935459","dateCreated":"2022-05-24T19:06:10+0000","dateStarted":"2022-05-24T20:13:08+0000","dateFinished":"2022-05-24T20:13:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56520"},{"text":"%md\n-- Here we are just selecting two columns from our dataset --> The happiness rank and happiness score","user":"anonymous","dateUpdated":"2022-05-24T20:13:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Here we are just selecting two columns from our dataset &ndash;&gt; The happiness rank and happiness score</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653423005994_1204997004","id":"20220524-201005_601234822","dateCreated":"2022-05-24T20:10:05+0000","dateStarted":"2022-05-24T20:13:08+0000","dateFinished":"2022-05-24T20:13:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56521"},{"text":"val rank_score = data.select(col(\"Happiness Rank\").cast(DoubleType), col(\"Happiness Score\").cast(DoubleType))","user":"anonymous","dateUpdated":"2022-05-24T20:13:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rank_score: org.apache.spark.sql.DataFrame = [Happiness Rank: double, Happiness Score: double]\n"}]},"apps":[],"jobName":"paragraph_1653419263250_-1719440433","id":"20220524-190743_550259917","dateCreated":"2022-05-24T19:07:43+0000","dateStarted":"2022-05-24T20:13:08+0000","dateFinished":"2022-05-24T20:13:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56522"},{"text":"rank_score.show(5)","user":"anonymous","dateUpdated":"2022-05-24T20:13:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+---------------+\n|Happiness Rank|Happiness Score|\n+--------------+---------------+\n|           1.0|    7.808700085|\n|           2.0|    7.645599842|\n|           3.0|    7.559899807|\n|           4.0|    7.504499912|\n|           5.0|    7.487999916|\n+--------------+---------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=187"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653423025337_1621765777","id":"20220524-201025_378953462","dateCreated":"2022-05-24T20:10:25+0000","dateStarted":"2022-05-24T20:13:09+0000","dateFinished":"2022-05-24T20:13:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56523"},{"text":"%md\r\n-- Next we split the dataset into training and test sets\r\n-- We use a randomSplit function with a split percentage of 80%, and 20%\r\n-- The second argument to the function is the seed, it is used to get the same random results every time (for reproducibility of the results)","user":"anonymous","dateUpdated":"2022-05-24T20:13:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Next we split the dataset into training and test sets<br/>&ndash; We use a randomSplit function with a split percentage of 80%, and 20%<br/>&ndash; The second argument to the function is the seed, it is used to get the same random results every time (for reproducibility of the results)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653422450037_1694822384","id":"20220524-200050_91780530","dateCreated":"2022-05-24T20:00:50+0000","dateStarted":"2022-05-24T20:13:09+0000","dateFinished":"2022-05-24T20:13:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56524"},{"text":"val Array(trainingData, testData) = rank_score.randomSplit(Array(0.8, 0.2), seed=1111) ","user":"anonymous","dateUpdated":"2022-05-24T20:13:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Happiness Rank: double, Happiness Score: double]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Happiness Rank: double, Happiness Score: double]\n"}]},"apps":[],"jobName":"paragraph_1653419470160_-1999053411","id":"20220524-191110_1590887882","dateCreated":"2022-05-24T19:11:10+0000","dateStarted":"2022-05-24T20:13:09+0000","dateFinished":"2022-05-24T20:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56525"},{"text":"%md\n## Machine Learning Steps\n\n-- All the following steps will be the same for all machine learning problems\n\n-- Preparing features and labels \n\n-- We need to prepare our features and labels to supply it to our algorithm, in this case linear regression\n-- Remember features are the input data to our algorithm and labels are the values our algorithm is going to train the model to predict\n-- Features are also called independent variables\n-- Lables are also called dependent variables\n\n-- In our problem, we only have one FEATURE, it is the happiness score\n-- We will predict the happiness rank, which becomes the LABEL\n\n-- We will pass an array to the InputCols with the name of all the features \n-- And we will simply give the name of the output column\n\n-- Think of a VectorAssembler as an Array\n-- Vector assembler can be used to package one feature or multiple features into a vector\n-- And this will be our features column\n-- We pass an array to inputCols with the name of all the features we want to assemple","user":"anonymous","dateUpdated":"2022-05-24T20:13:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Machine Learning Steps</h2>\n<p>&ndash; All the following steps will be the same for all machine learning problems</p>\n<p>&ndash; Preparing features and labels </p>\n<p>&ndash; We need to prepare our features and labels to supply it to our algorithm, in this case linear regression<br/>&ndash; Remember features are the input data to our algorithm and labels are the values our algorithm is going to train the model to predict<br/>&ndash; Features are also called independent variables<br/>&ndash; Lables are also called dependent variables</p>\n<p>&ndash; In our problem, we only have one FEATURE, it is the happiness score<br/>&ndash; We will predict the happiness rank, which becomes the LABEL</p>\n<p>&ndash; We will pass an array to the InputCols with the name of all the features<br/>&ndash; And we will simply give the name of the output column</p>\n<p>&ndash; Think of a VectorAssembler as an Array<br/>&ndash; Vector assembler can be used to package one feature or multiple features into a vector<br/>&ndash; And this will be our features column<br/>&ndash; We pass an array to inputCols with the name of all the features we want to assemple</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419481123_-1018197674","id":"20220524-191121_427623199","dateCreated":"2022-05-24T19:11:21+0000","dateStarted":"2022-05-24T20:13:10+0000","dateFinished":"2022-05-24T20:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56526"},{"text":"val assembler = new VectorAssembler()\n.setInputCols(Array(\"Happiness Score\"))\n.setOutputCol(\"assembled-features\")","user":"anonymous","dateUpdated":"2022-05-24T20:13:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_1289373bb3b0\n"}]},"apps":[],"jobName":"paragraph_1653419528929_-58286108","id":"20220524-191208_943865217","dateCreated":"2022-05-24T19:12:08+0000","dateStarted":"2022-05-24T20:13:10+0000","dateFinished":"2022-05-24T20:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56527"},{"text":"%md\r\n-- Next we will instantiate our algorithm, in this case linear regression, and set the features and label column\r\n-- In this case, the features column will be the output from VectorAssembler which is the assembled features\r\n-- And label is the value linear regression will try to predict which is the happiness rank\r\n-- Features column will be the output form the vector assempler","user":"anonymous","dateUpdated":"2022-05-24T20:13:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Next we will instantiate our algorithm, in this case linear regression, and set the features and label column<br/>&ndash; In this case, the features column will be the output from VectorAssembler which is the assembled features<br/>&ndash; And label is the value linear regression will try to predict which is the happiness rank<br/>&ndash; Features column will be the output form the vector assempler</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419545910_105596451","id":"20220524-191225_1407448953","dateCreated":"2022-05-24T19:12:25+0000","dateStarted":"2022-05-24T20:13:10+0000","dateFinished":"2022-05-24T20:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56528"},{"text":"val lr = new LinearRegression() \n .setFeaturesCol(\"assembled-features\")\n .setLabelCol(\"Happiness Rank\")\n","user":"anonymous","dateUpdated":"2022-05-24T20:13:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lr: org.apache.spark.ml.regression.LinearRegression = linReg_4f4d858e33b9\n"}]},"apps":[],"jobName":"paragraph_1653419569737_1282062175","id":"20220524-191249_1342360705","dateCreated":"2022-05-24T19:12:49+0000","dateStarted":"2022-05-24T20:13:10+0000","dateFinished":"2022-05-24T20:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56529"},{"text":"%md\n-- Next we create a pipeline for everything that needs to be executed\n-- Think of the pipeline as the assembly line in the factory where all the parts for a product are assembled together\n-- This is what we are doing with pipeline\n-- We create STAGES in pipelines\n-- In this example, our pipeline has only two stages\n-- The first stage is where we assemble the features with VectorAssembler\n-- And the second stage, we'll specify the algorithm that is used to train the model\n","user":"anonymous","dateUpdated":"2022-05-24T20:21:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Next we create a pipeline for everything that needs to be executed<br/>&ndash; Think of the pipeline as the assembly line in the factory where all the parts for a product are assembled together<br/>&ndash; This is what we are doing with pipeline<br/>&ndash; We create STAGES in pipelines<br/>&ndash; In this example, our pipeline has only two stages<br/>&ndash; The first stage is where we assemble the features with VectorAssembler<br/>&ndash; And the second stage, we&rsquo;ll specify the algorithm that is used to train the model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419563390_322716657","id":"20220524-191243_2083502609","dateCreated":"2022-05-24T19:12:43+0000","dateStarted":"2022-05-24T20:21:14+0000","dateFinished":"2022-05-24T20:21:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56530"},{"text":"%spark\nval pipeline = new Pipeline()\n .setStages(Array(assembler, lr))","user":"anonymous","dateUpdated":"2022-05-24T20:13:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pipeline: org.apache.spark.ml.Pipeline = pipeline_0a2ad18b2c1f\n"}]},"apps":[],"jobName":"paragraph_1653419599010_148330134","id":"20220524-191319_1538358145","dateCreated":"2022-05-24T19:13:19+0000","dateStarted":"2022-05-24T20:13:11+0000","dateFinished":"2022-05-24T20:13:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56531"},{"text":"%md\n-- Once we train the model, we need to evaluate the model for accuracy\n-- Since this is a regression problem, we will use RegressionEvaluator to evaluate the model\n-- The model, once it performs the predictions, it will store the prediction results in the predition column\n-- Once a prediction is made, we need to evalue the prediction with the actual value, so we know how good the prediction is\n-- So in our case, our model will predict the ranking based on our score\n-- We need to see if the the prediction matches the actual or not\n-- If it doesn't match, we want to know how far the prediction is from the actual value\n-- We will use a metric called r squared to show the accuracy of the model prediction\n-- r squared is also called coefficient of determination\n-- r squared basically tells us how good our model is \n-- The higher the r2 model the better our model\n-- r2 is a value between 0 and 1, and as much as it is closer to 1 it is better)","user":"anonymous","dateUpdated":"2022-05-24T20:28:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Once we train the model, we need to evaluate the model for accuracy<br/>&ndash; Since this is a regression problem, we will use RegressionEvaluator to evaluate the model<br/>&ndash; The model, once it performs the predictions, it will store the prediction results in the predition column<br/>&ndash; Once a prediction is made, we need to evalue the prediction with the actual value, so we know how good the prediction is<br/>&ndash; So in our case, our model will predict the ranking based on our score<br/>&ndash; We need to see if the the prediction matches the actual or not<br/>&ndash; If it doesn&rsquo;t match, we want to know how far the prediction is from the actual value<br/>&ndash; We will use a metric called r squared to show the accuracy of the model prediction<br/>&ndash; r squared is also called coefficient of determination<br/>&ndash; r squared basically tells us how good our model is<br/>&ndash; The higher the r2 model the better our model<br/>&ndash; r2 is a value between 0 and 1, and as much as it is closer to 1 it is better)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419615012_765030804","id":"20220524-191335_1868375238","dateCreated":"2022-05-24T19:13:35+0000","dateStarted":"2022-05-24T20:28:55+0000","dateFinished":"2022-05-24T20:28:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56532"},{"text":"%spark\r\nval evaluator = new RegressionEvaluator()\r\n .setLabelCol(\"Happiness Rank\")\r\n .setPredictionCol(\"prediction\")\r\n .setMetricName(\"r2\")","user":"anonymous","dateUpdated":"2022-05-24T20:13:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_58abd20ddf03\n"}]},"apps":[],"jobName":"paragraph_1653419639713_966692930","id":"20220524-191359_945229384","dateCreated":"2022-05-24T19:13:59+0000","dateStarted":"2022-05-24T20:13:11+0000","dateFinished":"2022-05-24T20:13:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56533"},{"text":"%md\n-- Next we setup our cross validator\n-- cross validator is an interesting concept\n-- We need to provide two things to the CrossValidator\n-- An estimator and an evaluator\n-- Estimator we provide the pipeline\n-- For the evaluator we provide regression evaluator \n-- We can provide additinoal parameters called \"hyper parameters\" which can be used for tuning our model\n-- We don't have any hyper parameters so we create an instance of the parameter Grid Builder and call the build method\n-- This will provide an empty parameter map\n-- Finally we specify the fold as 3\n-- This means the cross validator will generate three sets of data from our initial training dataset\n-- In other words it \"folds\" the data into 3 and from each sets of data, it will use 2/3 of the data for training and 1/3 of the data for testing\n-- And then it will fix the model based on the best accuracy based on the defined evaluation metric, in our case r2","user":"anonymous","dateUpdated":"2022-05-24T20:33:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Next we setup our cross validator<br/>&ndash; cross validator is an interesting concept<br/>&ndash; We need to provide two things to the CrossValidator<br/>&ndash; An estimator and an evaluator<br/>&ndash; Estimator we provide the pipeline<br/>&ndash; For the evaluator we provide regression evaluator<br/>&ndash; We can provide additinoal parameters called &ldquo;hyper parameters&rdquo; which can be used for tuning our model<br/>&ndash; We don&rsquo;t have any hyper parameters so we create an instance of the parameter Grid Builder and call the build method<br/>&ndash; This will provide an empty parameter map<br/>&ndash; Finally we specify the fold as 3<br/>&ndash; This means the cross validator will generate three sets of data from our initial training dataset<br/>&ndash; In other words it &ldquo;folds&rdquo; the data into 3 and from each sets of data, it will use 2/3 of the data for training and 1/3 of the data for testing<br/>&ndash; And then it will fix the model based on the best accuracy based on the defined evaluation metric, in our case r2</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419655797_-40878922","id":"20220524-191415_1155686197","dateCreated":"2022-05-24T19:14:15+0000","dateStarted":"2022-05-24T20:13:11+0000","dateFinished":"2022-05-24T20:13:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56534"},{"text":"%spark\r\nval cross_validator = new CrossValidator()\r\n .setEstimator(pipeline)\r\n .setEvaluator(evaluator)\r\n .setEstimatorParamMaps(new ParamGridBuilder().build)\r\n .setNumFolds(3)","user":"anonymous","dateUpdated":"2022-05-24T20:13:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"cross_validator: org.apache.spark.ml.tuning.CrossValidator = cv_decbbb7d62c8\n"}]},"apps":[],"jobName":"paragraph_1653419680494_-81557796","id":"20220524-191440_773595662","dateCreated":"2022-05-24T19:14:40+0000","dateStarted":"2022-05-24T20:13:11+0000","dateFinished":"2022-05-24T20:13:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56535"},{"text":"%md\n-- Next we call fit on the cross validator passing our trainig dataset\n-- CrossValidator will now fold the dataset into 3, divide each subset into training and test set\n-- Inside each fold, it will use the training set to train and test set to evaluate the model based on r2\n-- The best of the three models is returned ","user":"anonymous","dateUpdated":"2022-05-24T20:35:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Next we call fit on the cross validator passing our trainig dataset<br/>&ndash; CrossValidator will now fold the dataset into 3, divide each subset into training and test set<br/>&ndash; Inside each fold, it will use the training set to train and test set to evaluate the model based on r2<br/>&ndash; The best of the three models is returned</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419685643_-1804084181","id":"20220524-191445_1792134176","dateCreated":"2022-05-24T19:14:45+0000","dateStarted":"2022-05-24T20:35:36+0000","dateFinished":"2022-05-24T20:35:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56536"},{"text":"%spark\nval cvModel = cross_validator.fit(trainingData)","user":"anonymous","dateUpdated":"2022-05-24T20:35:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_decbbb7d62c8\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=213","http://728940ef7f86:4040/jobs/job?id=214","http://728940ef7f86:4040/jobs/job?id=215","http://728940ef7f86:4040/jobs/job?id=216","http://728940ef7f86:4040/jobs/job?id=217","http://728940ef7f86:4040/jobs/job?id=218","http://728940ef7f86:4040/jobs/job?id=219","http://728940ef7f86:4040/jobs/job?id=220","http://728940ef7f86:4040/jobs/job?id=221","http://728940ef7f86:4040/jobs/job?id=222","http://728940ef7f86:4040/jobs/job?id=223","http://728940ef7f86:4040/jobs/job?id=224","http://728940ef7f86:4040/jobs/job?id=225","http://728940ef7f86:4040/jobs/job?id=226","http://728940ef7f86:4040/jobs/job?id=227","http://728940ef7f86:4040/jobs/job?id=228","http://728940ef7f86:4040/jobs/job?id=229","http://728940ef7f86:4040/jobs/job?id=230","http://728940ef7f86:4040/jobs/job?id=231","http://728940ef7f86:4040/jobs/job?id=232","http://728940ef7f86:4040/jobs/job?id=233","http://728940ef7f86:4040/jobs/job?id=234","http://728940ef7f86:4040/jobs/job?id=235"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653419687823_-61299729","id":"20220524-191447_862455991","dateCreated":"2022-05-24T19:14:47+0000","dateStarted":"2022-05-24T20:35:42+0000","dateFinished":"2022-05-24T20:35:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56537"},{"text":"%md\r\n-- Now cvModel is a trained model that we can use to make predictions\r\n-- We can actually save this model and reuse it as well\r\n-- We'll see how to do that later\r\n\r\n-- Before we can use our model on the real data, let's test our data on the real data to see how good it is\r\n-- We now called cvModel.transform on the testData set\r\n-- when we call transform, our model will go over the dataset and make predictions","user":"anonymous","dateUpdated":"2022-05-24T20:13:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Now cvModel is a trained model that we can use to make predictions<br/>&ndash; We can actually save this model and reuse it as well<br/>&ndash; We&rsquo;ll see how to do that later</p>\n<p>&ndash; Before we can use our model on the real data, let&rsquo;s test our data on the real data to see how good it is<br/>&ndash; We now called cvModel.transform on the testData set<br/>&ndash; when we call transform, our model will go over the dataset and make predictions</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419689488_2069815447","id":"20220524-191449_853834300","dateCreated":"2022-05-24T19:14:49+0000","dateStarted":"2022-05-24T20:13:14+0000","dateFinished":"2022-05-24T20:13:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56538"},{"text":"%spark\nval predictions = cvModel.transform(testData)","user":"anonymous","dateUpdated":"2022-05-24T20:41:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"predictions: org.apache.spark.sql.DataFrame = [Happiness Rank: double, Happiness Score: double ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1653419698386_-691693448","id":"20220524-191458_1860124094","dateCreated":"2022-05-24T19:14:58+0000","dateStarted":"2022-05-24T20:41:14+0000","dateFinished":"2022-05-24T20:41:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56539"},{"text":"%spark\npredictions.show()","user":"anonymous","dateUpdated":"2022-05-24T20:41:16+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+---------------+------------------+-------------------+\n|Happiness Rank|Happiness Score|assembled-features|         prediction|\n+--------------+---------------+------------------+-------------------+\n|           1.0|    7.808700085|     [7.808700085]|-14.526660087751509|\n|           4.0|    7.504499912|     [7.504499912]|-2.6485919803141655|\n|          10.0|    7.237500191|     [7.237500191]|  7.776914486129442|\n|          18.0|    6.939599991|     [6.939599991]| 19.408988294382027|\n|          21.0|    6.790800095|     [6.790800095]| 25.219160200719443|\n|          25.0|     6.45539999|      [6.45539999]|  38.31548835577158|\n|          33.0|    6.363399982|     [6.363399982]|  41.90780176615473|\n|          40.0|    6.227300167|     [6.227300167]| 47.222075114643644|\n|          45.0|     6.15899992|      [6.15899992]|  49.88898676567939|\n|          65.0|    5.747499943|     [5.747499943]|  65.95677760488744|\n|          80.0|    5.488800049|     [5.488800049]|  76.05820170999081|\n|          82.0|    5.384300232|     [5.384300232]|   80.1385936811707|\n|          84.0|    5.285600185|     [5.285600185]|  83.99252272043285|\n|          93.0|    5.131800175|     [5.131800175]|  89.99793348561522|\n|          96.0|    5.101500034|     [5.101500034]|   91.1810595006782|\n|         101.0|    4.980800152|     [4.980800152]|  95.89401349020682|\n|         108.0|    4.829299927|     [4.829299927]| 101.80962482301851|\n|         114.0|    4.729300022|     [4.729300022]| 105.71430926357357|\n|         128.0|    4.392199993|     [4.392199993]| 118.87701414961288|\n|         131.0|     4.31099987|      [4.31099987]| 122.04762573018647|\n+--------------+---------------+------------------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=242"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653424726555_-1085918159","id":"20220524-203846_919511183","dateCreated":"2022-05-24T20:38:46+0000","dateStarted":"2022-05-24T20:41:16+0000","dateFinished":"2022-05-24T20:41:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56540"},{"text":"%md\n-- Remember, the directory SHOULD NOT EXIST, remove it first if it does\n-- On HDFS:\nhadoop fs -rm -r /BigData/happiness/output/\n-- On local:\nrm -r /var/tmp/output\n-- Once the model has made predictions, we are just selecting the \"Happiness Rank\", \"Happiness Socre\" and the prediction column from the model\n-- and write it as a CSV file in a location in HDFS","user":"anonymous","dateUpdated":"2022-05-24T20:31:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Remember, the directory SHOULD NOT EXIST, remove it first if it does<br/>&ndash; On HDFS:<br/>hadoop fs -rm -r /BigData/happiness/output/<br/>&ndash; On local:<br/>rm -r /var/tmp/output<br/>&ndash; Once the model has made predictions, we are just selecting the &ldquo;Happiness Rank&rdquo;, &ldquo;Happiness Socre&rdquo; and the prediction column from the model<br/>&ndash; and write it as a CSV file in a location in HDFS</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653419700400_-1693317352","id":"20220524-191500_1941255526","dateCreated":"2022-05-24T19:15:00+0000","dateStarted":"2022-05-24T20:31:48+0000","dateFinished":"2022-05-24T20:31:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56541"},{"text":"%sh\nrm -r /var/tmp/output","user":"anonymous","dateUpdated":"2022-05-24T20:41:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1653420309633_-1366840886","id":"20220524-192509_1025607743","dateCreated":"2022-05-24T19:25:09+0000","dateStarted":"2022-05-24T20:41:23+0000","dateFinished":"2022-05-24T20:41:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56542"},{"text":"predictions\n .select(col(\"Happiness Rank\"), col(\"Happiness Score\"), col(\"prediction\"))\n .write\n .format(\"csv\")\n .save(\"file:///var/tmp/output/\")","user":"anonymous","dateUpdated":"2022-05-24T20:41:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=243"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653419702477_-791749276","id":"20220524-191502_1603173065","dateCreated":"2022-05-24T19:15:02+0000","dateStarted":"2022-05-24T20:41:24+0000","dateFinished":"2022-05-24T20:41:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56543"},{"text":"%md\n-- Finally, we call the evaluator method on the evaluator passing the preditions DataFrame\n-- It will give us back the r2 result\n-- R2 is presented as a percentage, if we get 0.9, means our model is 90% accurate","user":"anonymous","dateUpdated":"2022-05-24T20:13:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&ndash; Finally, we call the evaluator method on the evaluator passing the preditions DataFrame<br/>&ndash; It will give us back the r2 result<br/>&ndash; R2 is presented as a percentage, if we get 0.9, means our model is 90% accurate</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1653420087283_675017697","id":"20220524-192127_702290693","dateCreated":"2022-05-24T19:21:27+0000","dateStarted":"2022-05-24T20:13:15+0000","dateFinished":"2022-05-24T20:13:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56544"},{"text":"%spark\nval r2 = evaluator.evaluate(predictions)\nprintln(\"r-squared on test data = \" + r2)","user":"anonymous","dateUpdated":"2022-05-24T20:41:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"r-squared on test data = 0.9757969518906306\nr2: Double = 0.9757969518906306\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://728940ef7f86:4040/jobs/job?id=244"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1653420160378_712618238","id":"20220524-192240_317981401","dateCreated":"2022-05-24T19:22:40+0000","dateStarted":"2022-05-24T20:41:37+0000","dateFinished":"2022-05-24T20:41:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56545"}],"name":"Spark_Regression_Happiness","id":"2H6VGBVA1","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}